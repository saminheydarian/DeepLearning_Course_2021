{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_Learning_HW03.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPXTSeThdEWC"
      },
      "source": [
        "# Deep Learning 3rd Assignment - Part a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UPR9eNJdLZT"
      },
      "source": [
        "In this section you should implement Adam algorithm with numpy on the following objective function:\n",
        "\\begin{equation}\n",
        "x^2 + y^2\n",
        "\\end{equation}\n",
        "Try and analyze the performance of this algorithm with different values for beta1 and beta2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wcgz1SDzGZnl"
      },
      "source": [
        "from math import sqrt\n",
        "from numpy import asarray\n",
        "from numpy.random import rand\n",
        "from numpy.random import seed"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqo3LpzeD_Fl"
      },
      "source": [
        "# objective function\n",
        "def objective(x, y):\n",
        "\treturn x**2.0 + y**2.0"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PexYta6ZGV7f"
      },
      "source": [
        "# derivative of objective function\n",
        "def derivative(x, y):\n",
        "\treturn asarray([x * 2.0, y * 2.0])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sty_NE-mU5jO"
      },
      "source": [
        "# gradient descent algorithm with adam\n",
        "def adam(objective, derivative, bounds, n_iter, alpha, beta1, beta2, eps=1e-8):\n",
        "  # generate an initial point\n",
        "  x = bounds[:, 0] + rand(len(bounds)) * (bounds[:, 1] - bounds[:, 0])\n",
        "  score = objective(x[0], x[1])\n",
        "  # initialize first and second moments\n",
        "  m = [0.0 for _ in range(bounds.shape[0])]\n",
        "  v = [0.0 for _ in range(bounds.shape[0])]\n",
        "  # run the gradient descent updates\n",
        "  for t in range(n_iter):\n",
        "    # calculate gradient g(t)\n",
        "    g = derivative(x[0], x[1])\n",
        "    # build a solution one variable at a time\n",
        "    for i in range(x.shape[0]):\n",
        "      # m(t) = beta1 * m(t-1) + (1 - beta1) * g(t)\n",
        "      m[i] = beta1 * m[i] + (1.0 - beta1) * g[i]\n",
        "      # v(t) = beta2 * v(t-1) + (1 - beta2) * g(t)^2\n",
        "      v[i] = beta2 * v[i] + (1.0 - beta2) * g[i]**2\n",
        "      # mhat(t) = m(t) / (1 - beta1(t))\n",
        "      mhat = m[i] / (1.0 - beta1**(t+1))\n",
        "      # vhat(t) = v(t) / (1 - beta2(t))\n",
        "      vhat = v[i] / (1.0 - beta2**(t+1))\n",
        "      # x(t) = x(t-1) - alpha * mhat(t) / (sqrt(vhat(t)) + eps)\n",
        "      x[i] = x[i] - alpha * mhat / (sqrt(vhat) + eps)\n",
        "    # evaluate candidate point\n",
        "    score = objective(x[0], x[1])\n",
        "    # report progress\n",
        "    #print('>%d f(%s) = %.5f' % (t, x, score))\n",
        "  return [x, score]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7CTWPGdUvOH",
        "outputId": "3aed8718-4808-4fa6-eba7-49356a9f9835"
      },
      "source": [
        "# seed the pseudo random number generator\n",
        "seed(1)\n",
        "# define range for input\n",
        "bounds = asarray([[-1.0, 1.0], [-1.0, 1.0]])\n",
        "# define the total iterations\n",
        "n_iter = 1000\n",
        "# steps size\n",
        "alpha = 0.001\n",
        "# factor for average gradient and average squared gradient\n",
        "beta_list = [[0.9, 0.9],\n",
        "        [0.99, 0.9],\n",
        "        [0.999, 0.9],\n",
        "        [0.9, 0.99],\n",
        "        [0.99, 0.99],\n",
        "        [0.999, 0.99],\n",
        "        [0.9, 0.999],\n",
        "        [0.99, 0.999],\n",
        "        [0.999, 0.999],]\n",
        "print('Learning_rate: %s' % (alpha))\n",
        "print('\\n\\n')\n",
        "for betas in beta_list:\n",
        "  best, score = adam(objective, derivative, bounds, n_iter, alpha, betas[0], betas[1])\n",
        "  print('For beta1: ' + str(betas[0]) + ' and ' + 'beta2: ' + str(betas[1]) + ': ')\n",
        "  print('f(%s) = %f' % (best, score))\n",
        "  print('---------------------------------------------')\n",
        "  print()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning_rate: 0.001\n",
            "\n",
            "\n",
            "\n",
            "For beta1: 0.9 and beta2: 0.9: \n",
            "f([ 4.26158250e-05 -1.73075024e-05]) = 0.000000\n",
            "---------------------------------------------\n",
            "\n",
            "For beta1: 0.99 and beta2: 0.9: \n",
            "f([ 0.08044519 -0.00282757]) = 0.006479\n",
            "---------------------------------------------\n",
            "\n",
            "For beta1: 0.999 and beta2: 0.9: \n",
            "f([0.36538754 0.43164406]) = 0.319825\n",
            "---------------------------------------------\n",
            "\n",
            "For beta1: 0.9 and beta2: 0.99: \n",
            "f([-2.96270491e-04 -5.36789201e-14]) = 0.000000\n",
            "---------------------------------------------\n",
            "\n",
            "For beta1: 0.99 and beta2: 0.99: \n",
            "f([ 0.00088791 -0.00028316]) = 0.000001\n",
            "---------------------------------------------\n",
            "\n",
            "For beta1: 0.999 and beta2: 0.99: \n",
            "f([-0.0524686  -0.10088718]) = 0.012931\n",
            "---------------------------------------------\n",
            "\n",
            "For beta1: 0.9 and beta2: 0.999: \n",
            "f([-0.03549142  0.10466613]) = 0.012215\n",
            "---------------------------------------------\n",
            "\n",
            "For beta1: 0.99 and beta2: 0.999: \n",
            "f([-0.16632807  0.00290279]) = 0.027673\n",
            "---------------------------------------------\n",
            "\n",
            "For beta1: 0.999 and beta2: 0.999: \n",
            "f([-0.05642867 -0.0125135 ]) = 0.003341\n",
            "---------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5yUtdmGMRDQ",
        "outputId": "287dcf0b-7a5f-4f5e-8970-3b464f4eb27b"
      },
      "source": [
        "# seed the pseudo random number generator\n",
        "seed(1)\n",
        "# define range for input\n",
        "bounds = asarray([[-1.0, 1.0], [-1.0, 1.0]])\n",
        "# define the total iterations\n",
        "n_iter = 1000\n",
        "# steps size\n",
        "alpha = 0.01\n",
        "# factor for average gradient and average squared gradient\n",
        "beta_list = [[0.9, 0.9],\n",
        "        [0.99, 0.9],\n",
        "        [0.999, 0.9],\n",
        "        [0.9, 0.99],\n",
        "        [0.99, 0.99],\n",
        "        [0.999, 0.99],\n",
        "        [0.9, 0.999],\n",
        "        [0.99, 0.999],\n",
        "        [0.999, 0.999],]\n",
        "print('Learning_rate: %s' % (alpha))\n",
        "print('\\n\\n')\n",
        "for betas in beta_list:\n",
        "  best, score = adam(objective, derivative, bounds, n_iter, alpha, betas[0], betas[1])\n",
        "  print('For beta1: ' + str(betas[0]) + ' and ' + 'beta2: ' + str(betas[1]) + ': ')\n",
        "  print('f(%s) = %f' % (best, score))\n",
        "  print('---------------------------------------------')\n",
        "  print()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning_rate: 0.01\n",
            "\n",
            "\n",
            "\n",
            "For beta1: 0.9 and beta2: 0.9: \n",
            "f([0.00023262 0.00026493]) = 0.000000\n",
            "---------------------------------------------\n",
            "\n",
            "For beta1: 0.99 and beta2: 0.9: \n",
            "f([0.05046922 0.01534069]) = 0.002782\n",
            "---------------------------------------------\n",
            "\n",
            "For beta1: 0.999 and beta2: 0.9: \n",
            "f([-0.27458629 -0.40806127]) = 0.241912\n",
            "---------------------------------------------\n",
            "\n",
            "For beta1: 0.9 and beta2: 0.99: \n",
            "f([ 1.22019644e-22 -1.37968729e-26]) = 0.000000\n",
            "---------------------------------------------\n",
            "\n",
            "For beta1: 0.99 and beta2: 0.99: \n",
            "f([-0.00107206  0.00054226]) = 0.000001\n",
            "---------------------------------------------\n",
            "\n",
            "For beta1: 0.999 and beta2: 0.99: \n",
            "f([ 0.05311961 -0.02947737]) = 0.003691\n",
            "---------------------------------------------\n",
            "\n",
            "For beta1: 0.9 and beta2: 0.999: \n",
            "f([-3.45908234e-23 -8.02993979e-24]) = 0.000000\n",
            "---------------------------------------------\n",
            "\n",
            "For beta1: 0.99 and beta2: 0.999: \n",
            "f([-0.00344516  0.00130793]) = 0.000014\n",
            "---------------------------------------------\n",
            "\n",
            "For beta1: 0.999 and beta2: 0.999: \n",
            "f([0.01191707 0.00214377]) = 0.000147\n",
            "---------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-crMo4Hde-Vg"
      },
      "source": [
        "Please write your observations and conclusion of running the algorithm with different values for beta1 and beta2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZEmFRvkfQw4"
      },
      "source": [
        "<font color='red'>Please write your answer here.</font>\n",
        "\n",
        "wrote in report pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_aJl-g0lN8M"
      },
      "source": [
        "# Deep Learning 3rd Assignment - Part b\n",
        "\n",
        "---\n",
        "\n",
        "In this assignment we will be focusing on optimizing neural networks. As you may already know Keras API has a lot of built-in functions for optimization that you can use. However you may want to impelement your own custom optimization function or design one for your specific problem.\n",
        "\n",
        "The purpose of this notebook is to teach you to implement your own optimization function by using Keras API.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egTG4I4b1XvR"
      },
      "source": [
        "#@title Import necessary modules\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from keras.optimizers import Optimizer\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUFk7FaA1L-9"
      },
      "source": [
        "To keep things rather simple and draw our full attention towards implementing custom optimization functions, we use a simple classification problem. The first step is to prepare our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2n8keC22og1"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeFbGgzx2uXv"
      },
      "source": [
        "Run the code below to load and plot the handwritten digit recognition dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "C1Q_ukmlktUN",
        "outputId": "8f0edee0-cabc-4a32-fc29-bd81abfcffa6"
      },
      "source": [
        "# Load the dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "plt.subplot(221)\n",
        "plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
        "plt.subplot(222)\n",
        "plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n",
        "plt.subplot(223)\n",
        "plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n",
        "plt.subplot(224)\n",
        "plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TE7jDSnOH-i"
      },
      "source": [
        "Since we have not yet studied Convolutional Neural Networks (AKA CNN), we would like to solve this problem by using Multi-Layer Perceptrons. \n",
        "\n",
        "Note that our data is two-dimensional and since we are not using CNNs, we should reshape our data to make it compatible for MLPs. In the following code cell convert the input shape to a vector of pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pypr9kl4Qfg8"
      },
      "source": [
        "# Reshape the test and training datasets \n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2])\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])\n",
        "\n",
        "# Normalizing data\n",
        "\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARmIcHrRRRfi"
      },
      "source": [
        "Now since our problem is a multi-class classification problem we should label our data from 0 to 9 (there are 10 handwritten digits). We can simply do this using keras's built-in to_categorical() function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTHiVaQER1cc",
        "outputId": "2512e767-974c-4b76-9eaf-77cf0424a350"
      },
      "source": [
        "y_train = keras.utils.to_categorical(y_train)\n",
        "y_test = keras.utils.to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]\n",
        "\n",
        "print(num_classes)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mycw9cVoTlRt"
      },
      "source": [
        "## Defining Our Custom Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LicS7nyYTrz_"
      },
      "source": [
        "This is the main part of this assignment because you are going to implement your custom optimizer. \n",
        "\n",
        "Below you should implement the SGD algorithm that you have learned in class. The class inherits from the Optimizer class in Keras. So all you need to do is to overwrite the functions specified below to have your own custom optimizer. \n",
        "\n",
        "In the constructor section you should define your hyperparameters and in resource_apply_dence function you write your main algorithm for optimizing.\n",
        "\n",
        "You can visit [here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer) to get further information on how to implement your own optimizer in tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWBwHr7G_Mn6"
      },
      "source": [
        "class CustomSGD(Optimizer):\n",
        "  def __init__(self, learning_rate=0.01, name=\"CustomSGD\", **kwargs):\n",
        "      \"\"\"you can set and store your hyper-parameters here\"\"\"\n",
        "      super().__init__(name, **kwargs)\n",
        "      \"\"\"Define learning rate hyper param here\"\"\"\n",
        "      self._set_hyper(\"learning_rate\", kwargs.get(\"lr\", learning_rate)) # handle lr=learning_rate\n",
        "      self._set_hyper(\"decay\", self._initial_decay)\n",
        "\n",
        "\n",
        "\n",
        "  @tf.function\n",
        "  def _resource_apply_dense(self, grad, var):\n",
        "      \"\"\" This is where you implement SGD algorithm with decayed learning rate \"\"\"\n",
        "      var_dtype = var.dtype.base_dtype\n",
        "      lr_t = self._decayed_lr(var_dtype) # handle learning rate decay\n",
        "      var.assign_add(-grad * lr_t)\n",
        "\n",
        "  def _resource_apply_sparse(self, grad, var):\n",
        "      \"\"\" No need to do anything here \"\"\"\n",
        "      raise NotImplementedError\n",
        "\n",
        "  def get_config(self):\n",
        "      base_config = super().get_config()\n",
        "      return {\n",
        "          **base_config,\n",
        "          \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n",
        "          \"decay\": self._serialize_hyperparameter(\"decay\"),\n",
        "      }"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJPHYkn7S0Qi"
      },
      "source": [
        "## Creating Our Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOXiK6WwS7Te"
      },
      "source": [
        "In this section we will create our model. You only need dense layers and don't forget to use your custom SGD. Also try changing the learning rate of the optimizer and analyze the effect of the learning rate on your model's performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axYI8k-GSCV9"
      },
      "source": [
        "hidden_units = 128"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut2qNgjeTSru"
      },
      "source": [
        "def mlp_model(lrn_rate):\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.layers.Input(shape = X_train[0].shape))\n",
        "  model.add(keras.layers.Dense(units=hidden_units, activation='relu'))\n",
        "  model.add(keras.layers.Dense(units=10, activation='softmax'))\n",
        "\n",
        "  model.compile(optimizer=CustomSGD(learning_rate=lrn_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itvGZ8mE8bDw"
      },
      "source": [
        "lrn_rates = [0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFI65r3YZD3b"
      },
      "source": [
        "Now run the following code cell to train and evaluate our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgzRoV3PalVz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1311f2c5-2415-405a-a100-6488bbfbc33e"
      },
      "source": [
        "for lrn_rate in lrn_rates:\n",
        "  model = mlp_model(lrn_rate)\n",
        "\n",
        "  # Fit the model\n",
        "  model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
        "  # Final evaluation of the model\n",
        "  scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "  print()\n",
        "  print(\"Model Error for learning_rate %s: %.2f%%\" % (lrn_rate, 100-scores[1]*100))\n",
        "  print('##########################################################################################')\n",
        "  print()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "300/300 - 2s - loss: 2.3238 - accuracy: 0.1171 - val_loss: 2.1670 - val_accuracy: 0.2272\n",
            "Epoch 2/10\n",
            "300/300 - 1s - loss: 2.0494 - accuracy: 0.3670 - val_loss: 1.9327 - val_accuracy: 0.4775\n",
            "Epoch 3/10\n",
            "300/300 - 1s - loss: 1.8351 - accuracy: 0.5452 - val_loss: 1.7275 - val_accuracy: 0.5975\n",
            "Epoch 4/10\n",
            "300/300 - 1s - loss: 1.6436 - accuracy: 0.6313 - val_loss: 1.5435 - val_accuracy: 0.6608\n",
            "Epoch 5/10\n",
            "300/300 - 1s - loss: 1.4739 - accuracy: 0.6885 - val_loss: 1.3827 - val_accuracy: 0.7141\n",
            "Epoch 6/10\n",
            "300/300 - 1s - loss: 1.3274 - accuracy: 0.7297 - val_loss: 1.2458 - val_accuracy: 0.7538\n",
            "Epoch 7/10\n",
            "300/300 - 1s - loss: 1.2036 - accuracy: 0.7621 - val_loss: 1.1312 - val_accuracy: 0.7819\n",
            "Epoch 8/10\n",
            "300/300 - 1s - loss: 1.1003 - accuracy: 0.7828 - val_loss: 1.0362 - val_accuracy: 0.7996\n",
            "Epoch 9/10\n",
            "300/300 - 1s - loss: 1.0147 - accuracy: 0.7985 - val_loss: 0.9574 - val_accuracy: 0.8133\n",
            "Epoch 10/10\n",
            "300/300 - 1s - loss: 0.9434 - accuracy: 0.8105 - val_loss: 0.8918 - val_accuracy: 0.8224\n",
            "\n",
            "Model Error for learning_rate 0.001: 17.76%\n",
            "##########################################################################################\n",
            "\n",
            "Epoch 1/10\n",
            "300/300 - 1s - loss: 1.4390 - accuracy: 0.6515 - val_loss: 0.8729 - val_accuracy: 0.8125\n",
            "Epoch 2/10\n",
            "300/300 - 1s - loss: 0.7164 - accuracy: 0.8380 - val_loss: 0.5742 - val_accuracy: 0.8658\n",
            "Epoch 3/10\n",
            "300/300 - 1s - loss: 0.5388 - accuracy: 0.8677 - val_loss: 0.4705 - val_accuracy: 0.8831\n",
            "Epoch 4/10\n",
            "300/300 - 1s - loss: 0.4633 - accuracy: 0.8812 - val_loss: 0.4171 - val_accuracy: 0.8900\n",
            "Epoch 5/10\n",
            "300/300 - 1s - loss: 0.4203 - accuracy: 0.8881 - val_loss: 0.3836 - val_accuracy: 0.8986\n",
            "Epoch 6/10\n",
            "300/300 - 1s - loss: 0.3916 - accuracy: 0.8944 - val_loss: 0.3605 - val_accuracy: 0.9026\n",
            "Epoch 7/10\n",
            "300/300 - 1s - loss: 0.3707 - accuracy: 0.8990 - val_loss: 0.3436 - val_accuracy: 0.9056\n",
            "Epoch 8/10\n",
            "300/300 - 1s - loss: 0.3545 - accuracy: 0.9019 - val_loss: 0.3299 - val_accuracy: 0.9095\n",
            "Epoch 9/10\n",
            "300/300 - 1s - loss: 0.3411 - accuracy: 0.9053 - val_loss: 0.3181 - val_accuracy: 0.9114\n",
            "Epoch 10/10\n",
            "300/300 - 1s - loss: 0.3299 - accuracy: 0.9077 - val_loss: 0.3095 - val_accuracy: 0.9151\n",
            "\n",
            "Model Error for learning_rate 0.01: 8.49%\n",
            "##########################################################################################\n",
            "\n",
            "Epoch 1/10\n",
            "300/300 - 2s - loss: 0.5366 - accuracy: 0.8623 - val_loss: 0.3105 - val_accuracy: 0.9143\n",
            "Epoch 2/10\n",
            "300/300 - 1s - loss: 0.2930 - accuracy: 0.9175 - val_loss: 0.2596 - val_accuracy: 0.9304\n",
            "Epoch 3/10\n",
            "300/300 - 1s - loss: 0.2457 - accuracy: 0.9310 - val_loss: 0.2208 - val_accuracy: 0.9373\n",
            "Epoch 4/10\n",
            "300/300 - 1s - loss: 0.2150 - accuracy: 0.9396 - val_loss: 0.1961 - val_accuracy: 0.9449\n",
            "Epoch 5/10\n",
            "300/300 - 1s - loss: 0.1919 - accuracy: 0.9460 - val_loss: 0.1823 - val_accuracy: 0.9478\n",
            "Epoch 6/10\n",
            "300/300 - 1s - loss: 0.1740 - accuracy: 0.9503 - val_loss: 0.1671 - val_accuracy: 0.9525\n",
            "Epoch 7/10\n",
            "300/300 - 1s - loss: 0.1591 - accuracy: 0.9550 - val_loss: 0.1536 - val_accuracy: 0.9553\n",
            "Epoch 8/10\n",
            "300/300 - 1s - loss: 0.1461 - accuracy: 0.9583 - val_loss: 0.1444 - val_accuracy: 0.9596\n",
            "Epoch 9/10\n",
            "300/300 - 1s - loss: 0.1354 - accuracy: 0.9619 - val_loss: 0.1352 - val_accuracy: 0.9624\n",
            "Epoch 10/10\n",
            "300/300 - 1s - loss: 0.1260 - accuracy: 0.9650 - val_loss: 0.1290 - val_accuracy: 0.9625\n",
            "\n",
            "Model Error for learning_rate 0.1: 3.75%\n",
            "##########################################################################################\n",
            "\n",
            "Epoch 1/10\n",
            "300/300 - 1s - loss: 0.4330 - accuracy: 0.8806 - val_loss: 0.2550 - val_accuracy: 0.9278\n",
            "Epoch 2/10\n",
            "300/300 - 1s - loss: 0.2370 - accuracy: 0.9327 - val_loss: 0.1989 - val_accuracy: 0.9442\n",
            "Epoch 3/10\n",
            "300/300 - 1s - loss: 0.1859 - accuracy: 0.9470 - val_loss: 0.1663 - val_accuracy: 0.9512\n",
            "Epoch 4/10\n",
            "300/300 - 1s - loss: 0.1552 - accuracy: 0.9559 - val_loss: 0.1460 - val_accuracy: 0.9587\n",
            "Epoch 5/10\n",
            "300/300 - 1s - loss: 0.1337 - accuracy: 0.9625 - val_loss: 0.1287 - val_accuracy: 0.9637\n",
            "Epoch 6/10\n",
            "300/300 - 1s - loss: 0.1176 - accuracy: 0.9669 - val_loss: 0.1147 - val_accuracy: 0.9676\n",
            "Epoch 7/10\n",
            "300/300 - 1s - loss: 0.1052 - accuracy: 0.9704 - val_loss: 0.1086 - val_accuracy: 0.9683\n",
            "Epoch 8/10\n",
            "300/300 - 1s - loss: 0.0946 - accuracy: 0.9740 - val_loss: 0.1017 - val_accuracy: 0.9703\n",
            "Epoch 9/10\n",
            "300/300 - 1s - loss: 0.0860 - accuracy: 0.9760 - val_loss: 0.0969 - val_accuracy: 0.9717\n",
            "Epoch 10/10\n",
            "300/300 - 1s - loss: 0.0789 - accuracy: 0.9778 - val_loss: 0.0932 - val_accuracy: 0.9719\n",
            "\n",
            "Model Error for learning_rate 0.2: 2.81%\n",
            "##########################################################################################\n",
            "\n",
            "Epoch 1/10\n",
            "300/300 - 2s - loss: 0.3901 - accuracy: 0.8880 - val_loss: 0.2245 - val_accuracy: 0.9365\n",
            "Epoch 2/10\n",
            "300/300 - 1s - loss: 0.1984 - accuracy: 0.9431 - val_loss: 0.1649 - val_accuracy: 0.9510\n",
            "Epoch 3/10\n",
            "300/300 - 1s - loss: 0.1506 - accuracy: 0.9571 - val_loss: 0.1388 - val_accuracy: 0.9582\n",
            "Epoch 4/10\n",
            "300/300 - 1s - loss: 0.1230 - accuracy: 0.9649 - val_loss: 0.1162 - val_accuracy: 0.9664\n",
            "Epoch 5/10\n",
            "300/300 - 1s - loss: 0.1036 - accuracy: 0.9708 - val_loss: 0.1034 - val_accuracy: 0.9697\n",
            "Epoch 6/10\n",
            "300/300 - 1s - loss: 0.0901 - accuracy: 0.9748 - val_loss: 0.0950 - val_accuracy: 0.9692\n",
            "Epoch 7/10\n",
            "300/300 - 1s - loss: 0.0796 - accuracy: 0.9775 - val_loss: 0.0889 - val_accuracy: 0.9717\n",
            "Epoch 8/10\n",
            "300/300 - 1s - loss: 0.0707 - accuracy: 0.9801 - val_loss: 0.0855 - val_accuracy: 0.9743\n",
            "Epoch 9/10\n",
            "300/300 - 1s - loss: 0.0638 - accuracy: 0.9819 - val_loss: 0.0867 - val_accuracy: 0.9732\n",
            "Epoch 10/10\n",
            "300/300 - 1s - loss: 0.0579 - accuracy: 0.9835 - val_loss: 0.0774 - val_accuracy: 0.9764\n",
            "\n",
            "Model Error for learning_rate 0.3: 2.36%\n",
            "##########################################################################################\n",
            "\n",
            "Epoch 1/10\n",
            "300/300 - 2s - loss: 0.3646 - accuracy: 0.8927 - val_loss: 0.1896 - val_accuracy: 0.9449\n",
            "Epoch 2/10\n",
            "300/300 - 1s - loss: 0.1745 - accuracy: 0.9494 - val_loss: 0.1449 - val_accuracy: 0.9562\n",
            "Epoch 3/10\n",
            "300/300 - 1s - loss: 0.1285 - accuracy: 0.9632 - val_loss: 0.1153 - val_accuracy: 0.9671\n",
            "Epoch 4/10\n",
            "300/300 - 1s - loss: 0.1033 - accuracy: 0.9704 - val_loss: 0.1003 - val_accuracy: 0.9694\n",
            "Epoch 5/10\n",
            "300/300 - 1s - loss: 0.0864 - accuracy: 0.9754 - val_loss: 0.0911 - val_accuracy: 0.9725\n",
            "Epoch 6/10\n",
            "300/300 - 1s - loss: 0.0741 - accuracy: 0.9790 - val_loss: 0.0826 - val_accuracy: 0.9750\n",
            "Epoch 7/10\n",
            "300/300 - 1s - loss: 0.0648 - accuracy: 0.9818 - val_loss: 0.0819 - val_accuracy: 0.9741\n",
            "Epoch 8/10\n",
            "300/300 - 1s - loss: 0.0574 - accuracy: 0.9840 - val_loss: 0.0751 - val_accuracy: 0.9767\n",
            "Epoch 9/10\n",
            "300/300 - 1s - loss: 0.0512 - accuracy: 0.9861 - val_loss: 0.0743 - val_accuracy: 0.9765\n",
            "Epoch 10/10\n",
            "300/300 - 1s - loss: 0.0456 - accuracy: 0.9875 - val_loss: 0.0702 - val_accuracy: 0.9792\n",
            "\n",
            "Model Error for learning_rate 0.4: 2.08%\n",
            "##########################################################################################\n",
            "\n",
            "Epoch 1/10\n",
            "300/300 - 2s - loss: 0.3395 - accuracy: 0.8987 - val_loss: 0.1751 - val_accuracy: 0.9475\n",
            "Epoch 2/10\n",
            "300/300 - 1s - loss: 0.1547 - accuracy: 0.9546 - val_loss: 0.1255 - val_accuracy: 0.9630\n",
            "Epoch 3/10\n",
            "300/300 - 1s - loss: 0.1138 - accuracy: 0.9674 - val_loss: 0.1064 - val_accuracy: 0.9672\n",
            "Epoch 4/10\n",
            "300/300 - 1s - loss: 0.0910 - accuracy: 0.9739 - val_loss: 0.0915 - val_accuracy: 0.9741\n",
            "Epoch 5/10\n",
            "300/300 - 1s - loss: 0.0758 - accuracy: 0.9779 - val_loss: 0.0888 - val_accuracy: 0.9719\n",
            "Epoch 6/10\n",
            "300/300 - 1s - loss: 0.0645 - accuracy: 0.9814 - val_loss: 0.0819 - val_accuracy: 0.9746\n",
            "Epoch 7/10\n",
            "300/300 - 1s - loss: 0.0564 - accuracy: 0.9838 - val_loss: 0.0744 - val_accuracy: 0.9782\n",
            "Epoch 8/10\n",
            "300/300 - 1s - loss: 0.0488 - accuracy: 0.9859 - val_loss: 0.0809 - val_accuracy: 0.9748\n",
            "Epoch 9/10\n",
            "300/300 - 1s - loss: 0.0431 - accuracy: 0.9877 - val_loss: 0.0682 - val_accuracy: 0.9791\n",
            "Epoch 10/10\n",
            "300/300 - 1s - loss: 0.0381 - accuracy: 0.9896 - val_loss: 0.0686 - val_accuracy: 0.9787\n",
            "\n",
            "Model Error for learning_rate 0.5: 2.13%\n",
            "##########################################################################################\n",
            "\n",
            "Epoch 1/10\n",
            "300/300 - 2s - loss: 0.3409 - accuracy: 0.8973 - val_loss: 0.1617 - val_accuracy: 0.9493\n",
            "Epoch 2/10\n",
            "300/300 - 1s - loss: 0.1408 - accuracy: 0.9588 - val_loss: 0.1177 - val_accuracy: 0.9650\n",
            "Epoch 3/10\n",
            "300/300 - 1s - loss: 0.1026 - accuracy: 0.9700 - val_loss: 0.1088 - val_accuracy: 0.9646\n",
            "Epoch 4/10\n",
            "300/300 - 1s - loss: 0.0806 - accuracy: 0.9760 - val_loss: 0.0930 - val_accuracy: 0.9706\n",
            "Epoch 5/10\n",
            "300/300 - 1s - loss: 0.0672 - accuracy: 0.9804 - val_loss: 0.0793 - val_accuracy: 0.9751\n",
            "Epoch 6/10\n",
            "300/300 - 1s - loss: 0.0566 - accuracy: 0.9836 - val_loss: 0.0752 - val_accuracy: 0.9757\n",
            "Epoch 7/10\n",
            "300/300 - 1s - loss: 0.0486 - accuracy: 0.9860 - val_loss: 0.0761 - val_accuracy: 0.9763\n",
            "Epoch 8/10\n",
            "300/300 - 1s - loss: 0.0429 - accuracy: 0.9878 - val_loss: 0.0675 - val_accuracy: 0.9802\n",
            "Epoch 9/10\n",
            "300/300 - 1s - loss: 0.0372 - accuracy: 0.9894 - val_loss: 0.0718 - val_accuracy: 0.9778\n",
            "Epoch 10/10\n",
            "300/300 - 1s - loss: 0.0323 - accuracy: 0.9912 - val_loss: 0.0706 - val_accuracy: 0.9773\n",
            "\n",
            "Model Error for learning_rate 0.6: 2.27%\n",
            "##########################################################################################\n",
            "\n",
            "Epoch 1/10\n",
            "300/300 - 2s - loss: 0.3398 - accuracy: 0.8960 - val_loss: 0.1517 - val_accuracy: 0.9532\n",
            "Epoch 2/10\n",
            "300/300 - 1s - loss: 0.1339 - accuracy: 0.9600 - val_loss: 0.1117 - val_accuracy: 0.9655\n",
            "Epoch 3/10\n",
            "300/300 - 1s - loss: 0.0981 - accuracy: 0.9709 - val_loss: 0.0926 - val_accuracy: 0.9718\n",
            "Epoch 4/10\n",
            "300/300 - 1s - loss: 0.0777 - accuracy: 0.9761 - val_loss: 0.0844 - val_accuracy: 0.9741\n",
            "Epoch 5/10\n",
            "300/300 - 1s - loss: 0.0633 - accuracy: 0.9808 - val_loss: 0.0872 - val_accuracy: 0.9721\n",
            "Epoch 6/10\n",
            "300/300 - 1s - loss: 0.0544 - accuracy: 0.9836 - val_loss: 0.0809 - val_accuracy: 0.9752\n",
            "Epoch 7/10\n",
            "300/300 - 1s - loss: 0.0461 - accuracy: 0.9865 - val_loss: 0.0717 - val_accuracy: 0.9783\n",
            "Epoch 8/10\n",
            "300/300 - 1s - loss: 0.0398 - accuracy: 0.9883 - val_loss: 0.0739 - val_accuracy: 0.9771\n",
            "Epoch 9/10\n",
            "300/300 - 1s - loss: 0.0340 - accuracy: 0.9905 - val_loss: 0.0752 - val_accuracy: 0.9768\n",
            "Epoch 10/10\n",
            "300/300 - 1s - loss: 0.0303 - accuracy: 0.9912 - val_loss: 0.0723 - val_accuracy: 0.9783\n",
            "\n",
            "Model Error for learning_rate 0.7: 2.17%\n",
            "##########################################################################################\n",
            "\n",
            "Epoch 1/10\n",
            "300/300 - 1s - loss: 0.3432 - accuracy: 0.8948 - val_loss: 0.1496 - val_accuracy: 0.9545\n",
            "Epoch 2/10\n",
            "300/300 - 1s - loss: 0.1341 - accuracy: 0.9596 - val_loss: 0.1232 - val_accuracy: 0.9631\n",
            "Epoch 3/10\n",
            "300/300 - 1s - loss: 0.0982 - accuracy: 0.9704 - val_loss: 0.0983 - val_accuracy: 0.9699\n",
            "Epoch 4/10\n",
            "300/300 - 1s - loss: 0.0789 - accuracy: 0.9756 - val_loss: 0.0947 - val_accuracy: 0.9712\n",
            "Epoch 5/10\n",
            "300/300 - 1s - loss: 0.0649 - accuracy: 0.9801 - val_loss: 0.0825 - val_accuracy: 0.9731\n",
            "Epoch 6/10\n",
            "300/300 - 1s - loss: 0.0556 - accuracy: 0.9826 - val_loss: 0.0812 - val_accuracy: 0.9740\n",
            "Epoch 7/10\n",
            "300/300 - 1s - loss: 0.0476 - accuracy: 0.9853 - val_loss: 0.0797 - val_accuracy: 0.9752\n",
            "Epoch 8/10\n",
            "300/300 - 1s - loss: 0.0413 - accuracy: 0.9873 - val_loss: 0.0812 - val_accuracy: 0.9743\n",
            "Epoch 9/10\n",
            "300/300 - 1s - loss: 0.0355 - accuracy: 0.9894 - val_loss: 0.0773 - val_accuracy: 0.9775\n",
            "Epoch 10/10\n",
            "300/300 - 1s - loss: 0.0298 - accuracy: 0.9914 - val_loss: 0.0734 - val_accuracy: 0.9779\n",
            "\n",
            "Model Error for learning_rate 0.8: 2.21%\n",
            "##########################################################################################\n",
            "\n",
            "Epoch 1/10\n",
            "300/300 - 1s - loss: 0.3527 - accuracy: 0.8913 - val_loss: 0.2340 - val_accuracy: 0.9218\n",
            "Epoch 2/10\n",
            "300/300 - 1s - loss: 0.1336 - accuracy: 0.9595 - val_loss: 0.1045 - val_accuracy: 0.9682\n",
            "Epoch 3/10\n",
            "300/300 - 1s - loss: 0.0977 - accuracy: 0.9708 - val_loss: 0.0913 - val_accuracy: 0.9735\n",
            "Epoch 4/10\n",
            "300/300 - 1s - loss: 0.0769 - accuracy: 0.9769 - val_loss: 0.0861 - val_accuracy: 0.9737\n",
            "Epoch 5/10\n",
            "300/300 - 1s - loss: 0.0646 - accuracy: 0.9807 - val_loss: 0.0786 - val_accuracy: 0.9756\n",
            "Epoch 6/10\n",
            "300/300 - 1s - loss: 0.0547 - accuracy: 0.9835 - val_loss: 0.0797 - val_accuracy: 0.9757\n",
            "Epoch 7/10\n",
            "300/300 - 1s - loss: 0.0464 - accuracy: 0.9862 - val_loss: 0.0788 - val_accuracy: 0.9772\n",
            "Epoch 8/10\n",
            "300/300 - 1s - loss: 0.0391 - accuracy: 0.9883 - val_loss: 0.0787 - val_accuracy: 0.9755\n",
            "Epoch 9/10\n",
            "300/300 - 1s - loss: 0.0350 - accuracy: 0.9894 - val_loss: 0.0719 - val_accuracy: 0.9790\n",
            "Epoch 10/10\n",
            "300/300 - 1s - loss: 0.0292 - accuracy: 0.9915 - val_loss: 0.0837 - val_accuracy: 0.9745\n",
            "\n",
            "Model Error for learning_rate 0.9: 2.55%\n",
            "##########################################################################################\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}